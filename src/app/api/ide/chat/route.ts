import { NextRequest, NextResponse } from 'next/server'
import { getAuthFromRequest } from '@/lib/auth'
import { getGroqClient } from '@/lib/groq'
import { getOpenRouterClient } from '@/lib/openrouter'
import { rateLimit, RATE_LIMITS } from '@/lib/rate-limit'

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// POST /api/ide/chat â€” Multi-provider AI chat for the IDE
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface ChatMessage {
    role: 'system' | 'user' | 'assistant'
    content: string
}

// â”€â”€ Model â†’ Provider routing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
type Provider = 'groq' | 'openrouter'

const MODEL_PROVIDER_MAP: Record<string, { provider: Provider; modelId: string }> = {
    // â”€â”€ Groq (native, fast) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    'llama-3.3-70b-versatile': { provider: 'groq', modelId: 'llama-3.3-70b-versatile' },
    'llama-3.1-8b-instant': { provider: 'groq', modelId: 'llama-3.1-8b-instant' },
    'llama-3.2-90b-text-preview': { provider: 'groq', modelId: 'llama-3.2-90b-text-preview' },
    'llama-3.2-11b-text-preview': { provider: 'groq', modelId: 'llama-3.2-11b-text-preview' },
    'gemma2-9b-it': { provider: 'groq', modelId: 'gemma2-9b-it' },
    'mixtral-8x7b-32768': { provider: 'groq', modelId: 'mixtral-8x7b-32768' },
    'deepseek-r1-distill-llama-70b': { provider: 'groq', modelId: 'deepseek-r1-distill-llama-70b' },
    'qwen-2.5-coder-32b': { provider: 'groq', modelId: 'qwen-qwq-32b' },

    // â”€â”€ Claude (Anthropic via OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    'claude-sonnet-4': { provider: 'openrouter', modelId: 'anthropic/claude-sonnet-4-20250514' },
    'claude-haiku-3.5': { provider: 'openrouter', modelId: 'anthropic/claude-3.5-haiku' },
    'claude-opus-4': { provider: 'openrouter', modelId: 'anthropic/claude-opus-4-20250514' },
    'claude-sonnet-3.5': { provider: 'openrouter', modelId: 'anthropic/claude-3.5-sonnet' },

    // â”€â”€ GPT (OpenAI via OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    'gpt-4o': { provider: 'openrouter', modelId: 'openai/gpt-4o' },
    'gpt-4o-mini': { provider: 'openrouter', modelId: 'openai/gpt-4o-mini' },
    'gpt-4.1': { provider: 'openrouter', modelId: 'openai/gpt-4.1' },
    'gpt-4.1-mini': { provider: 'openrouter', modelId: 'openai/gpt-4.1-mini' },
    'o3-mini': { provider: 'openrouter', modelId: 'openai/o3-mini' },

    // â”€â”€ Gemini (Google via OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    'gemini-2.5-pro': { provider: 'openrouter', modelId: 'google/gemini-2.5-pro-preview' },
    'gemini-2.5-flash': { provider: 'openrouter', modelId: 'google/gemini-2.5-flash-preview' },
    'gemini-2.0-flash': { provider: 'openrouter', modelId: 'google/gemini-2.0-flash-001' },

    // â”€â”€ DeepSeek (via OpenRouter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    'deepseek-v3': { provider: 'openrouter', modelId: 'deepseek/deepseek-chat-v3-0324' },
    'deepseek-r1': { provider: 'openrouter', modelId: 'deepseek/deepseek-r1' },
}

const IDE_SYSTEM_PROMPT = `VocÃª Ã© Sofia, uma assistente de programaÃ§Ã£o de IA integrada Ã  IDE Sofia.

Suas capacidades:
- Analisar e explicar cÃ³digo em qualquer linguagem
- Sugerir refatoraÃ§Ãµes e melhorias
- Encontrar bugs e problemas potenciais
- Escrever testes unitÃ¡rios
- Gerar documentaÃ§Ã£o
- Responder dÃºvidas sobre programaÃ§Ã£o

Regras:
- Sempre responda em portuguÃªs (Brasil)
- Quando sugerir mudanÃ§as no cÃ³digo, use blocos de cÃ³digo markdown com a linguagem destacada
- Seja conciso mas completo
- Se o usuÃ¡rio compartilhar cÃ³digo, analise-o cuidadosamente antes de responder
- Quando houver contexto de arquivo, mencione o nome do arquivo e a linguagem
- Use emojis moderadamente para tornar a conversa mais amigÃ¡vel
- Prefira sugestÃµes prÃ¡ticas a explicaÃ§Ãµes teÃ³ricas longas`

export async function POST(request: NextRequest) {
    try {
        const auth = await getAuthFromRequest(request)
        if (!auth) {
            return NextResponse.json({ error: 'NÃ£o autenticado' }, { status: 401 })
        }

        // Rate limit by user ID
        const rl = rateLimit(`ide_chat_${auth.id}`, RATE_LIMITS.ideChat.max, RATE_LIMITS.ideChat.window)
        if (!rl.allowed) {
            return NextResponse.json(
                { error: 'Rate limit exceeded. Please wait before sending more messages.' },
                { status: 429, headers: { 'Retry-After': String(Math.ceil((rl.resetAt - Date.now()) / 1000)) } }
            )
        }

        const body = await request.json()
        const {
            message,
            history = [],
            fileContext,
            model = 'llama-3.3-70b-versatile',
        } = body as {
            message: string
            history: ChatMessage[]
            fileContext?: {
                fileName: string
                filePath: string
                language: string
                content: string
                selection?: string
            }
            model?: string
        }

        if (!message?.trim()) {
            return NextResponse.json({ error: 'Mensagem obrigatÃ³ria' }, { status: 400 })
        }

        // Resolve provider and model ID
        const routing = MODEL_PROVIDER_MAP[model]
        if (!routing) {
            return NextResponse.json({ error: `Modelo "${model}" nÃ£o suportado` }, { status: 400 })
        }

        // Build messages
        const messages: ChatMessage[] = [
            { role: 'system', content: IDE_SYSTEM_PROMPT },
        ]

        // Add file context if provided
        if (fileContext) {
            const contextMsg = [
                `ðŸ“„ **Arquivo ativo:** \`${fileContext.fileName}\` (${fileContext.language})`,
                `ðŸ“ **Caminho:** \`${fileContext.filePath}\``,
            ]

            if (fileContext.selection) {
                contextMsg.push(`\n**CÃ³digo selecionado:**\n\`\`\`${fileContext.language}\n${fileContext.selection}\n\`\`\``)
            } else if (fileContext.content) {
                const lines = fileContext.content.split('\n')
                const truncated = lines.length > 200
                    ? lines.slice(0, 200).join('\n') + '\n// ... (truncado)'
                    : fileContext.content
                contextMsg.push(`\n**ConteÃºdo do arquivo:**\n\`\`\`${fileContext.language}\n${truncated}\n\`\`\``)
            }

            messages.push({
                role: 'system',
                content: `Contexto do editor:\n${contextMsg.join('\n')}`,
            })
        }

        // Add conversation history (last 20 messages)
        const recentHistory = history.slice(-20)
        for (const msg of recentHistory) {
            messages.push({
                role: msg.role === 'user' ? 'user' : 'assistant',
                content: msg.content,
            })
        }

        // Add current message
        messages.push({ role: 'user', content: message })

        // â”€â”€ Route to correct provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        let reply: string

        if (routing.provider === 'groq') {
            const groq = getGroqClient()
            const completion = await groq.chat.completions.create({
                messages,
                model: routing.modelId,
                temperature: 0.3,
                max_tokens: 2048,
            })
            reply = completion.choices[0]?.message?.content || 'Desculpe, nÃ£o consegui gerar uma resposta.'
        } else {
            // OpenRouter
            const openrouter = getOpenRouterClient()
            const completion = await openrouter.chat.completions.create({
                messages,
                model: routing.modelId,
                temperature: 0.3,
                max_tokens: 4096,
            })
            reply = completion.choices[0]?.message?.content || 'Desculpe, nÃ£o consegui gerar uma resposta.'
        }

        return NextResponse.json({
            success: true,
            message: {
                role: 'assistant',
                content: reply,
            },
        })
    } catch (error) {
        console.error('[IDE Chat] Error:', error)
        return NextResponse.json(
            { error: 'Erro ao processar mensagem' },
            { status: 500 }
        )
    }
}
